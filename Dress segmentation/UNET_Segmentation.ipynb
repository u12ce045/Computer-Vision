{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d35b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Basic Python Library \"\"\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Deep Learning Library\"\"\"\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision as T\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138360a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available else 'cpu') #cuda:0 0 for indicating which gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb195fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original=cv2.imread('original77.jpg')\n",
    "original=cv2.resize(original,(224,224))\n",
    "dress=cv2.imread('dress77.jpg')\n",
    "dress=cv2.resize(dress,(224,224))\n",
    "body=cv2.imread('body77.jpg')\n",
    "body=cv2.resize(body,(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('Original')\n",
    "plt.imshow(cv2.cvtColor(original,cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(3,1,2)\n",
    "plt.title('Dress')\n",
    "plt.imshow(cv2.cvtColor(dress,cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(3,1,3)\n",
    "plt.title('Body')\n",
    "plt.imshow(cv2.cvtColor(body,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dress_gray=cv2.cvtColor(dress,cv2.COLOR_BGR2GRAY)\n",
    "body_gray=cv2.cvtColor(body,cv2.COLOR_BGR2GRAY)\n",
    "print(np.unique(dress_gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd23cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Dress and Body\n",
    "dress_gray=np.where(dress_gray<255,255,0)\n",
    "body_gray=np.where(body_gray<255,255,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136784b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('Dress in gray format')\n",
    "plt.imshow(dress_gray)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Body in gray format')\n",
    "plt.imshow(body_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_gray=body_gray-dress_gray\n",
    "plt.imshow(skin_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.title('Person/Background')\n",
    "body_gray = (255 - body_gray)/255\n",
    "plt.imshow(body_gray)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Skin')\n",
    "skin_gray = (255 - skin_gray)/255\n",
    "plt.imshow(skin_gray)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Dress')\n",
    "dress_gray = (255 - dress_gray)/255\n",
    "plt.imshow(dress_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of segmenting image\n",
    "combine=np.zeros((224,224,3))\n",
    "combine[:,:,0]=(1-skin_gray)\n",
    "combine[:,:,1]=(1-dress_gray)\n",
    "combine[:,:,2]=body_gray\n",
    "plt.imshow(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91260006",
   "metadata": {},
   "source": [
    "### Above method is simplest procedure to segmenting the objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24985e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "\n",
    "class DressCollection(torch.utils.data.Dataset):\n",
    "    def __init__(self,root,transform1,transform2):\n",
    "        self.original=os.listdir(root+'/original')\n",
    "        self.original_path=glob(os.path.join(root,self.original)+'/*')\n",
    "        self.body=os.listdir(root+'/body')\n",
    "        self.body_path=glob(os.path.join(root,self.body)+'/*')\n",
    "        self.dress=os.lisdir(root+'/dress')\n",
    "        self.dress_path=glob(os.path.join(root,self.dress)+'/*')\n",
    "        self.transform1=transform1\n",
    "        self.transform2=transform2\n",
    "    def __len__(self):\n",
    "        return len(self.original_path)\n",
    "    def __getitem__(self,idx):\n",
    "        original_image=self.transform1(Image.open(self.original_path[idx]))\n",
    "        dress_image=cv2.imread(self.dress_path[idx])\n",
    "        body_image=cv2.imread(self.body_path[idx])\n",
    "        \n",
    "        dress_image=cv2.resize(dress_image,(224,244))\n",
    "        body_image=cv2.resize(body_image,(224,224))\n",
    "        \n",
    "        dress_image=np.where(dress_image<255,255,0)\n",
    "        body_image=np.where(body_image<255,255,0)\n",
    "        \n",
    "        skin = body - dress\n",
    "        bg = (255 - body)/255\n",
    "        skin = (255 - skin)/255\n",
    "        dress = (255 - dress)/255\n",
    "        \n",
    "        gt = np.zeros((224,224,3))\n",
    "        gt[:,:,0] = (1-skin)\n",
    "        gt[:,:,1] = (1-dress)\n",
    "        gt[:,:,2] = bg\n",
    "    \n",
    "        gt = np.zeros((224,224,3))\n",
    "        gt[:,:,0] = (1-skin)\n",
    "        gt[:,:,1] = (1-dress)\n",
    "        gt[:,:,2] = bg\n",
    "        \n",
    "        return original_image,self.transform(gt)\n",
    "\n",
    "root='Addres of the Dataset stored'\n",
    "transform1=T.transforms.Compose([T.transforms.ToTensor(),T.transforms.Resize(224)])\n",
    "transfrom2=T.transforms.Compose([T.transforms.ToTensor()])\n",
    "\n",
    "datasets=DressCollection(root,transform1,transform2)\n",
    "\n",
    "original_images=torch.stack([imf for img,_ in datasets])\n",
    "rgb_mean=origina_images.view(3,-1).mean(dim=1)\n",
    "# Normalize the images to speed up the calculation\n",
    "transform1=T.transforms.Compose([T.transforms.ToTensor(),\\\n",
    "                                 T.Normalize(mean=rgb_mmean,std=(1.0,1.0,1.0),\\\n",
    "                                 T.transforms.Resize(224)])\n",
    "        \n",
    "# Now again do the datasets methods\n",
    "                                 \n",
    "datasets=DressCollection(root,transform1,transform2)\n",
    "BATCH_SIZE=32\n",
    "dataloader=torch.utils.data.DataLoader(datasets,batch_size=BATCH_SIZE,shuffle=True)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c179c5c",
   "metadata": {},
   "source": [
    "## UNET architecture first developed for biomedical application, now its use in global purpose and one is semantic segmentation of an image\n",
    "\n",
    "### Below is the link provided for implementing a UNET model using pytorch DL framework.\n",
    "[Unet 1](https://www.youtube.com/watch?v=u1loyDCoGbE)\n",
    "[Unet 2](https://www.youtube.com/watch?v=IHq1t7NxS8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b951302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This method is credited to Abhishek Thakur  \"\"\"\n",
    "# You can use predefined model also\n",
    "def downsample(in_channels,out_channels):\n",
    "    conv=nn.Sequential(nn.Conv2d(in_channels,out_channels,3,1,1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout2d(0.4),\n",
    "                                 nn.Conv2d(out_channels,out_channels,3,1,1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout2d(0.4))\n",
    "        \n",
    "    return conv\n",
    "def concat_upsample(in_channels,out_channels,present_conv,previous_conv):\n",
    "    up=nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2,stride=2,padding=0)\n",
    "    value=torch.cat([up(present_conv),previous_conv],dim=1)\n",
    "    conv=nn.Sequential(nn.Conv2d(value.size(1),out_channels,kernel_size=3,stride=1,padding=1),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Dropout2d(0.4),\n",
    "                       nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Dropout2d(0.4))\n",
    "    return conv(value)\n",
    "    \n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet,self).__init__()\n",
    "        self.conv1=downsample(3,32)\n",
    "        self.conv2=downsample(32,64)\n",
    "        self.conv3=downsample(64,128)\n",
    "        self.conv4=downsample(128,256)\n",
    "        self.conv5=downsample(256,512)\n",
    "        \n",
    "       \n",
    "        self.endlayer=nn.Conv2d(32,3,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        def forward(self,image):\n",
    "        x1=self.conv1(image)\n",
    "#        print(x1.size())\n",
    "        p1=F.max_pool2d(x1,kernel_size=(2,2))\n",
    "#        print(p1.size())\n",
    "        x2=self.conv2(p1)\n",
    "#        print(x2.size())\n",
    "        p2=F.max_pool2d(x2,kernel_size=(2,2))\n",
    "#        print(p2.size())\n",
    "        x3=self.conv3(p2)\n",
    "#        print(x3.size())\n",
    "        p3=F.max_pool2d(x3,kernel_size=(2,2))\n",
    "#        print(p3.size())\n",
    "        x4=self.conv4(p3)\n",
    "#        print(x4.size())\n",
    "        p4=F.max_pool2d(x4,kernel_size=(2,2))\n",
    "#        print(p4.size())\n",
    "        x5=self.conv5(p4)\n",
    "#        print(x5.size())\n",
    "        x6=concat_upsample(512,256,x5,x4)\n",
    "#        print(x6.size())\n",
    "        x7=concat_upsample(256,128,x6,x3)\n",
    "#        print(x7.size())\n",
    "        x8=concat_upsample(128,64,x7,x2)\n",
    "#        print(x8.size())\n",
    "        x9=concat_upsample(64,32,x8,x1)\n",
    "#        print(x9.size())\n",
    "        x10=torch.sigmoid(self.endlayer(x9))\n",
    "        return x10\n",
    "        \n",
    "model=Unet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "gamma=2.0\n",
    "alpha=0.25\n",
    "def focal_loss(gamma,alpha,inputs,targets,logits=False):\n",
    "    if logits:\n",
    "        bce_loss=F.binary_cross_entropy_with_logits(inputs, targets,reduction='none')\n",
    "    else:\n",
    "        bce_loss=F.binary_cross_entropy(inputs, targets,reduction='none')\n",
    "    pt=torch.exp(-bce_loss)\n",
    "    focal_loss=alpha*(1-pt)**gamma*bce_loss\n",
    "    return torch.mean(focal_loss)\n",
    "summary(model,input_size=(3,224,224),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a986a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "loss_per_epoch=[]\n",
    "EPOCHS=100\n",
    "for e in tqdm.tqdm(range(EPOCHS)):\n",
    "    loss=0.0\n",
    "    model.train()\n",
    "    for raw,gt in dataloader:\n",
    "        raw,gt=Variable(raw.to(device)),Variable(gt.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        outs=model(raw)\n",
    "        loss_fn=focal_loss(outs,gt)\n",
    "        loss+=loss_fn.cpu().item()*gt.size(0)\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()\n",
    "    loss_per_epoch.append(loss/len(dataloader.sampler))\n",
    "    print('Epoch_',e,'_lOSS:',loss_per_epoch[e])\n",
    "#%%\n",
    "\"\"\" Save and load your model \"\"\" \n",
    "torch.save(model.state_dict(),root+'/unet_model.pth')\n",
    "model=Unet()\n",
    "model.load_state_dict(torch.load(root+'/unet_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2d218",
   "metadata": {},
   "source": [
    "## Grabcut Algorithm\n",
    "[grabcut cv2](https://www.pyimagesearch.com/2020/07/27/opencv-grabcut-foreground-segmentation-and-extraction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of the image\n",
    "def grab_cut(image):\n",
    "    image=cv2.resize(image,(224,224))\n",
    "    image = cv2.resize(image,(224,224))\n",
    "    \n",
    "    mask = np.zeros(image.shape[:2],np.uint8)\n",
    "    bgdModel = np.zeros((1,65),np.float64)\n",
    "    fgdModel = np.zeros((1,65),np.float64)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    rect = (50,10,width-100,height-20)\n",
    "    cv2.grabCut(image,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    img2 = image*mask2[:,:,np.newaxis]\n",
    "    img2[mask2 == 0] = (255, 255, 255)\n",
    "    final = np.ones(image.shape,np.uint8)*0 + img2\n",
    "    \n",
    "    return mask, final\n",
    "#%%\n",
    "### READ NEW IMAGE ###\n",
    "plt.figure(figsize=(16,8))\n",
    "image = cv2.imread('Any test image format .jpg, .png')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(cv2.cvtColor(cv2.resize(image.copy(),(224,224)), cv2.COLOR_BGRA2RGB))\n",
    "\n",
    "### GRUBCUT + PREDICTION ###\n",
    "mask_test, test = grab_cut(image)\n",
    "test = transform1(test)\n",
    "if torch.cuda.is_available():\n",
    "    test=test.to(device)\n",
    "pred = model(test.unsqueeze(0))\n",
    "pred=pred.detach().cpu().numpy().squeeze(0)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
